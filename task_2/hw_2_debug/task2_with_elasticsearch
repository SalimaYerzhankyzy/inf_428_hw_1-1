import numpy as np
import unittest
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk


# Function to generate random threat data
def generate_random_data(mean, variance, num_samples):
    return np.random.randint(max(mean - variance, 0), min(mean + variance + 1, 90), num_samples)


# Function to compute the department's average threat score
def compute_department_score(threat_scores):
    return np.mean(threat_scores)


# Function to compute the overall company's aggregated threat score
def compute_aggregated_threat_score(departments):
    department_scores = []

    for department in departments:
        users = department['users']
        variance_threat = np.var(users)
        dept_score = compute_department_score(users)
        department_scores.append(dept_score + variance_threat)

    # Weighted average of department scores
    aggregated_score = np.mean(department_scores)
    return min(max(0, int(aggregated_score)), 90)


# Initialize Elasticsearch client
es = Elasticsearch("http://localhost:9200")
INDEX_NAME = "threat_scores"

# Step 1: Create Elasticsearch index with mappings
def create_index():
    if not es.indices.exists(index=INDEX_NAME):
        mappings = {
            "mappings": {
                "properties": {
                    "department": {"type": "keyword"},
                    "user_id": {"type": "integer"},
                    "score": {"type": "float"}
                }
            }
        }
        es.indices.create(index=INDEX_NAME, body=mappings)
        print(f"Index '{INDEX_NAME}' created.")
    else:
        print(f"Index '{INDEX_NAME}' already exists.")


# Step 2: Generate random data and save to CSV
def generate_and_save_data():
    csv_file = "threat_scores.csv"
    if not os.path.exists(csv_file):
        data = []
        departments = ["Engineering", "Marketing", "Finance", "HR", "Science"]
        for department in departments:
            for user_id in range(np.random.randint(10, 200)):
                score = np.random.uniform(0, 90)  # Generate random scores
                data.append([department, user_id, score])

        # Save data to CSV
        with open(csv_file, mode='w', newline='') as file:
            writer = csv.writer(file)
            writer.writerow(["department", "user_id", "score"])
            writer.writerows(data)
        print("Data saved to CSV.")
    else:
        print("CSV file already exists.")


# Step 3: Populate Elasticsearch index from CSV
def populate_index_from_csv():
    csv_file = "threat_scores.csv"
    if os.path.exists(csv_file):
        with open(csv_file, mode='r') as file:
            reader = csv.DictReader(file)
            actions = [
                {
                    "_index": INDEX_NAME,
                    "_source": {
                        "department": row["department"],
                        "user_id": int(row["user_id"]),
                        "score": float(row["score"])
                    }
                }
                for row in reader
            ]
        bulk(es, actions)
        print("Elasticsearch index populated.")
    else:
        print("CSV file not found. Generate data first.")


# Step 4: Read data from Elasticsearch for calculations
def fetch_department_scores():
    query = {
        "size": 10000,
        "query": {
            "match_all": {}
        }
    }
    results = es.search(index=INDEX_NAME, body=query)
    data = {}
    for hit in results["hits"]["hits"]:
        department = hit["_source"]["department"]
        score = hit["_source"]["score"]
        if department not in data:
            data[department] = []
        data[department].append(score)
    return data


def aggregated_threat_score_from_es():
    department_scores = fetch_department_scores()
    aggregated_scores = []

    for department, scores in department_scores.items():
        mean_threat = np.mean(scores)
        variance_threat = np.var(scores)
        aggregated_scores.append(mean_threat + variance_threat)

    aggregated_score = np.mean(aggregated_scores)
    return min(max(0, int(aggregated_score)), 90)


# Functional test case using unittest
class TestCyberSecurity(unittest.TestCase):

    def setUp(self):
        # Setup basic department parameters for test cases
        self.departments = ['Engineering', 'Marketing', 'Finance', 'HR', 'Science']
        create_index()
        generate_and_save_data()
        populate_index_from_csv()

    def test__no_outliers_same_threat_score(self):
        """ Case where all departments have no outliers, similar threat scores """
        departments = [
            {'users': generate_random_data(45, 5, 100)} for _ in self.departments
        ]

        departments[2] = {'users': generate_random_data(50, 5, 100)}

        # Compute the aggregated threat score
        aggregated_score = compute_aggregated_threat_score(departments)
        print(f"Aggregated Score (no outliers): {aggregated_score}")
        self.assertTrue(40 <= aggregated_score <= 60, f"Aggregated score is out of range: {aggregated_score}")

    def test__high_outlier_department(self):
        """ Case where one department has significantly higher threat score than others """
        departments = [
            {'users': generate_random_data(45, 10, 100)} for _ in self.departments
        ]

        departments[3] = {'users': generate_random_data(80, 5, 100)}

        # Compute the aggregated threat score
        aggregated_score = compute_aggregated_threat_score(departments)
        print(f"Aggregated Score (high outlier): {aggregated_score}")
        self.assertTrue(50 <= aggregated_score <= 90, f"Aggregated score is out of range: {aggregated_score}")

    def test__varied_department_sizes(self):
        """ Case where departments have varying  threat score distributions """
        departments = [
            {'users': generate_random_data(45, 10, 50)},  # Small department
            {'users': generate_random_data(40, 5, 200)},  # Large department
            {'users': generate_random_data(50, 15, 100)},  # Medium department
            {'users': generate_random_data(35, 5, 150)},  # Medium-large department
            {'users': generate_random_data(80, 20, 30)}  # Very small department
        ]

        # Compute the aggregated threat score
        aggregated_score = compute_aggregated_threat_score(departments)
        print(f"Aggregated Score (varying size): {aggregated_score}")
        self.assertTrue(50 <= aggregated_score <= 90, f"Aggregated score is out of range: {aggregated_score}")

    def test_extreme_outliers(self):
        """ Case where some departments have extreme outliers """
        departments = [
            {'users': generate_random_data(40, 10, 100)} for _ in self.departments
        ]
        # Add extreme outliers to one department
        departments[4]['users'] = np.append(departments[3]['users'], [90, 90, 90, 90, 90])

        # Compute the aggregated threat score
        aggregated_score = compute_aggregated_threat_score(departments)
        print(f"Aggregated Score (extreme outlier): {aggregated_score}")
        self.assertTrue(50 <= aggregated_score <= 90, f"Aggregated score is out of range: {aggregated_score}")

    def test_with_elasticsearch(self):
        """ Test aggregated threat score using Elasticsearch data """
        score = aggregated_threat_score_from_es()
        print(f"Aggregated Score from Elasticsearch: {score}")
        self.assertTrue(0 <= score <= 90, f"Aggregated score from Elasticsearch is out of range: {score}")


# Run the unit tests
if __name__ == '__main__':
    unittest.main()
